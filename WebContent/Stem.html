<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>TheAlexSunWebsite</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="icon" type="image/ico" href="Images/favicon.ico">
</head>
<body>
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container-fluid">
      <ul class="nav navbar-nav navbar-left">
        <li><a href="./index.html">Home</a></li>
        <li><a href="./About.html">About</a></li>
        <li><a href="./Interests.html">Interests</a></li>
        <li><a href="./CommunityService.html">Community Service</a></li>
        <li class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#">Academics<span class="caret"></span></a>
        <ul class="dropdown-menu">
          <li><a href="./CS.html">CS</a></li>
          <li class="active"><a href="./Stem.html">STEM</a></li>
          <li><a href="./STW.html">STW</a></li>
          <li><a href="./Stem2.html">STEM2</a></li>
          <li><a href="./Math.html">Math</a></li>
          <li><a href="./Humanities.html">Humanities</a></li>
          <li><a href="./Language.html">Language</a></li>
          <li><a href="./Physics.html">Physics</a></li>
        </ul>
      </ul>
    </div>
  </nav>
  <div id="boringImage">
    <div class="hero-text">
      <h1>STEM</h1>
    </div>
  </div>
  <div class="container">
    <div class="standard">
      <p>STEM is the centerpiece of all Mass Academy Classes. It represents the blood, sweat, especially tears, and the individual strength and ability of each student here at MAMS.
        STEM is a class where each student works on their own individual science fair projects, where each student chooses from their own interests to research.
      </p>
    </div>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-md-6">
        <h3 class="standard">
          Phrase 1:
        </h3>
        <p class="smallCenter">
          People with hearing impairments are unable to sense their environment outside of their field of vision, which leaves them in danger of missing crucial sounds that could pose a danger in their everyday lives.
        </p>
      </div>
      <div class="col-md-6">
        <h3 class="standard">
          Phrase 2:
        </h3>
        <p class="smallCenter">
          The overall aim of this project is to engineer a wearable device that allows the hearing-impaired to gain more awareness by identifying the direction of incoming sound with availability for all, low cost, short latency and minimal invasiveness.
        </p>
      </div>
    </div>
  </div>
  <div class="container">
  <div class="row">
    <div class="standard">
      <p>
        Machine Learning in Sound Source Localization for the Hearing Impaired
      </p>
    </div>
  </div>
  <div class="container">
    <div class="standard">
      <h1>
        Abstract:
      </h1>
      <p>
        Over five percent of the world, or 466 million people, suffer from disabling hearing loss. Modern assistive devices are expensive and fragile, which makes them difficult to obtain and maintain for many. Furthermore, without assistive devices, even common environments become dangerous. The overall aim of this project is to engineer a wearable device that allows the hearing-impaired to gain more awareness by identifying the direction of incoming sound with high availability, low cost, short latency, and minimal invasiveness. To tackle this problem, a device was developed that utilizes a microphone array that could perceive time delay differences in incoming sound. A testing setup was created and used to automatically measure different time delays from different incidence angles to create a dataset. A Neural Network was trained to classify the sound’s direction using gradient based learning, which could then provide feedback to the user. The model was ported onto a small portable computer and incorporated into a small wearable device with a microphone array and vibration feedback belt for a disabled user to wear. The device has shown a low validation accuracy of 40.0% but excels in minimizing latency. The system shows promise in making predictions quickly, but improvements will have to be made before it will be viable for more delicate tasks.
      </p>
    </div>
  </div>
  <div class="container">
    <div class="standard">
      <h1>
        Background:
      </h1>
      <p>
        People with hearing impairments are unable to sense their environment outside of their field of vision which often leaves them with the danger of missing crucial noises that could pose a danger to their lives in public, and creates heavy communication barriers between other people and themselves. Over five percent of the world’s population faces some aspect of impaired hearing, and the cost of hearing aids and cochlear implants, the modern-day solutions to this problem, are unaffordable to the wide majority of the hearing impaired.
      </p>
      <p>
        Currently, around 466 million people suffer from disabling hearing loss, which is around five percent of the total world population. The main impact of disabling hearing loss is the communication barrier. Often, they are left isolated from others because they are socially ignored, and because the majority of the population fails to learn sign language which cuts off much of their communication. Feelings of isolation and loneliness surface as even when they are surrounded by others, they are ignored and treated as if they are invisible (“Deafness and Hearing Loss,” 2019). In families with hearing-impaired children, less than a third of the parents sign regularly, which creates a permanent social divide between even.
      </p>
      <p>
        Sound Source Localization (SSL) describes the use of acoustic technology to determine the location of sound in robots.  In humans, this capability is crucial as it allows for increased comprehension of speech, by allowing for separation between different sound sources. Furthermore, in the animal kingdom, this awareness allows for the rapid location of prey, making it a crucial capability for survival. Currently, researchers aim to recreate these systems for a variety of uses including sound source tracking, speech enhancement, virtual reality, and human-robot interaction (Risoud et al., 2018). Researchers are studying how cameras can use this technology to track different speakers during a meeting and recreate it virtually for a more immersive experience. They aim to create faster, more efficient, and smaller designs in order to reach their goal. SSL technology is evolving to meet modern needs and hopes of increased digital connection. Currently, societal standards and needs require improvements in processing speed, size, and computational cost, all while maintaining a similar level of accuracy and performance. The Internet of Things (IoT) is expanding quickly, and many want to have SSL capabilities. SSL improvements could better the safety of autonomous robots through safeguards (Zhao et al., 2012). Overall, SSL is quickly expanding fields intending to bridge the gap between human and robotic capabilities.
      </p>
    </div>
  </div>

  <div class="container">
    <div class="standard">
      <h1>
        Procedure:
      </h1>
      <p>
        The microphone array device was created with the attempt to be able to measure distinct time differences between the microphones in its systems. These measures also had to be systematic and in doing so, a symmetric microphone array had to be created. The design pursued was first modeled using the Onshape online CAD engine. This array was designed to hold four microphones in a square configuration evenly spaced with a total distance of 30.0cm between opposing microphones. Furthermore, a mounting system was created to attach to the stepper motor driven testing rig. This involved the creation of screw holes which attached onto the metal setting plate. The part was then 3D printed with a Creality CR-10s printer, using PLA filament.
      </p>
      <p>
        Next, the testing rig was designed to house the stepper motor, which created a cubical design around the motor and the motor sat internally in the chamber created. Feet were extended from the base so it could be attached strongly to the ground without movement. Then the speaker holding device was created with a height of 20.0cm to match the microphone array and mounted the speaker so it directly faced the microphones. Furthermore, feet were extended to lock it down to the ground and prevent movement with tape. The part was then 3D printed with a Creality CR-10s printer, using PLA filament as well.
      </p>
      <p>
        From there, the microcontrollers and electronics had to be added. First the stepper motor was added to the driver board and connected to the Arduino which controlled the board. The driver board was powered with a supplemental power supply of 12v, while the Arduino was plugged into the computers USB port. Then the Arduino was attached via serial cable to the central Raspberry Pi. The speaker was plugged into that Arduino as well, with a cable of length more than a meter. The microphone array had to be configured next. Each microphone was soldered into a custom solderable circuit board which allowed the transfer of the power supply to each breakout board. Then the analog outs were plugged into the microphone array Arduino so that they could be read. Finally, that Arduino was also plugged into a different port on the central Raspberry Pi. Thus, the hardware setup was finished.
      </p>
      <p>
      	The code for the Raspberry Pi was written in Python, and the code for the Arduinos was written in the Arduino Library for C++. The code utilizes serial connection to send bits between each system which is micromanaged by the Arduino. The Raspberry Pi sends signals to tell the testing rig to rotate and change the angle, and play a sound, and then it tells the microphone array record what was played on all four microphones. Then the data gets sent back into the Raspberry Pi where it is stored and labeled with its quadrant, eighth and step number.
      </p>
      <p>
        Finally, a machine learning model was trained using Tensorflow Keras, and multiple dense and convolutional layers. Then it was trained for 40 epochs with a batch size of 20. This created the inference, which was then ported back onto the Raspberry Pi so the microphone array could make inferences. Then a device to notify the user of the prediction form the Raspberry Pi was created, which consists of eight vibration disc motors each individually controlled by a relay. Then a separate Arduino gets sent Bluetooth signals from the microphone array Arduino using an HC-05 Bluetooth pair. This marks the design and creation of the final device.
      	The machine learning model’s accuracy was tested by separating the dataset into a training and validation set. The validating set never affects the training of the model but serves as a benchmark as to how accurately the model learned to generalize for cases outside its knowledge. This was the value measured for and compared between models.
      </p>
    </div>
  </div>

  <div class="container">
    <div class="standard">
      <h1>
        Results:
      </h1>
      <p>
        The created dataset marked 577 labels placed with four recordings each. The data took over an hour to record which resulted 2308 recordings, resulting in 5770000 total samples.
      </p>
      <p>
        The recorded validation accuracy of the neural network was this 40.0% for the quadrant model. The validation accuracy was measured by Tensorflow as the amount of predictions it got right over the total predictions made over the validation set. The validation set is a random sample taken from the dataset, which is never trained upon to measure the actual generalization ability of the model. The maximum normal accuracy of the model was 100.0%, showing the model memorized rather than generalizing for all cases.
        The model ran for 30 epochs with a learning rate of 0.001
      </p>
      <p>
        The weight of the overall device was 449.80g meaning it was rather light. The dimensions wher 26cm by 26cm by 20cm tall, meaning the volume was 13520 cubic centimeters which was measured by taking the largest measure in each dimension and measuring it like a rectangular prism.
      </p>
    </div>
  </div>
  <div class="container">
    <div class="standard">
      <h1>
        Discussion/Conclusion:
      </h1>
      <p>
        The results acquired from this project display many interesting findings. First, the validation accuracy of the machine learning model of the fourth’s classifications show that the Neural Network was unable to fully grasp and process data from this device properly. The device and sample rate were not small enough to limit this model’s ability to classify accurately. The maximum speed the Arduino could sample from the microphones was around 1250 samples per second, which, when taking into account the speed of sound, means that adjacent microphones at some points would have the sound appear in the same time slot, making it indistinguishable for the neural network. Furthermore, the noise that the microphones recorded, and just natural variation within it due to the noisy power supply meant random spikes, and unclean quiet spots. The speaker also failed to provide a large enough amplitude difference for the microphones to pick up, making it significantly harder for the neural networks to learn from. This is shown by how the neural network hit a normal accuracy of 100.0% by memorizing the exact cases there to provide the correct label, but when it predicted the generalization cases in the validation set, still failed terribly, hitting 40.0% at its maximum. Unfortunately, these results mean that the device would have to be much improved in order to be market viable in any way.
      </p>
      <p>
        On the other hand, the weight, and size of this device showed that it this system is applicable to wearable technology, and that it would be minimally invasive. If the other components were to function than the package size and weight would be very impressive, and likely the size would not be the limiting factor in the equation.
      </p>
      <p>
        Future extensions are plentiful as many improvements could be made to the device to increase the possibility of success. First off, a louder speaker or more sensitive microphones would be employed as a higher amplitude change would allow the neural network to find bigger landmarks to look for changes. Then, a different microcontroller like a teensy, or something that was clocked higher in order to take more samples per second, and store more onboard so the serial interface would not be such a bottleneck as it was in this project. Bigger landmarks would improve the overall capability of the neural network as convolutional neural networks specialize in finding these differences for classification. Very little other factors would need to be changed, as the entire structure of the device would stay the same. Furthermore the layers used on the neural would remain the same as well.
      </p>
      <p>
        In conclusion, this project displays the potential of machine learning in the field of Sound Source Localization as such primitive processing devices like Arduinos with next to no on-board memory can provide enough samples to create such profound results. Furthermore, it was unclear whether such a small microphone array would be capable of allowing enough differences to be analyzed, as most microphone arrays in the past have exceeded sizes of one square meter. This shows that for less precise requirements this type of device can function strongly and contribute to the field of Sound Source Localization.
      </p>
    </div>
  </div>
  <div>
    <h1>
      References
    </h1>
    <iframe src="./Docs/References.pdf" style="width:95%;height:800px"></iframe>
  </div>
  <div>
    <h1>
      Poster:
    </h1>
    <iframe src="./Docs/PosterStuff.pdf" style="width:95%;height:800px"></iframe
  </div>
</div>
</body>
</html>
